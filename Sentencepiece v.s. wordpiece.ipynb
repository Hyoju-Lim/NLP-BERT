{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d30e489",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "import pandas as pd\n",
    "import urllib.request\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24572040",
   "metadata": {},
   "source": [
    "### IMDB 리뷰 데이터\n",
    "- sentence-piece v.s. word-piece 비교"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5155337f",
   "metadata": {},
   "source": [
    "### 1) Sentence-piece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42fc9a04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('IMDb_Reviews.csv', <http.client.HTTPMessage at 0x1f97bc9e9a0>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/LawrenceDuan/IMDb-Review-Analysis/master/IMDb_Reviews.csv\", filename=\"IMDb_Reviews.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff00b216",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        My family and I normally do not watch local mo...\n",
       "1        Believe it or not, this was at one time the wo...\n",
       "2        After some internet surfing, I found the \"Home...\n",
       "3        One of the most unheralded great works of anim...\n",
       "4        It was the Sixties, and anyone with long hair ...\n",
       "                               ...                        \n",
       "49995    the people who came up with this are SICK AND ...\n",
       "49996    The script is so so laughable... this in turn,...\n",
       "49997    \"So there's this bride, you see, and she gets ...\n",
       "49998    Your mind will not be satisfied by this nobud...\n",
       "49999    The chaser's war on everything is a weekly sho...\n",
       "Name: review, Length: 50000, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv('IMDb_Reviews.csv')\n",
    "train_df['review']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "278ad096",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "리뷰 개수:  50000\n"
     ]
    }
   ],
   "source": [
    "# 총 50000개 샘플 존재\n",
    "print('리뷰 개수: ', len(train_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58df8f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence-piece의 입력으로 사용하기 위해 df를 txt로 저장\n",
    "with open('imdb_review.txt', 'w', encoding='utf8') as f:\n",
    "    f.write('\\n'.join(train_df['review']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2444d9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence-piece로 각 단어에 고유한 정수 부여 (vocab 생성 과정)\n",
    "### vocab size 5000으로 지정\n",
    "spm.SentencePieceTrainer.Train('--input=imdb_review.txt --model_prefix=imdb --vocab_size=5000 --model_type=bpe --max_sentence_length=9999')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00be6036",
   "metadata": {},
   "source": [
    "#### -> vocab 생성이 완료되면, imdb.model, imdb.vocab 파일 2개가 생성된"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "73bbba85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4485</th>\n",
       "      <td>▁unnecessary</td>\n",
       "      <td>-4482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4235</th>\n",
       "      <td>apped</td>\n",
       "      <td>-4232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2821</th>\n",
       "      <td>▁note</td>\n",
       "      <td>-2818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3561</th>\n",
       "      <td>IT</td>\n",
       "      <td>-3558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1127</th>\n",
       "      <td>▁comm</td>\n",
       "      <td>-1124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1232</th>\n",
       "      <td>▁song</td>\n",
       "      <td>-1229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2189</th>\n",
       "      <td>▁sequel</td>\n",
       "      <td>-2186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1788</th>\n",
       "      <td>ances</td>\n",
       "      <td>-1785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1192</th>\n",
       "      <td>▁disappoint</td>\n",
       "      <td>-1189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2509</th>\n",
       "      <td>Ch</td>\n",
       "      <td>-2506</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0     1\n",
       "4485  ▁unnecessary -4482\n",
       "4235         apped -4232\n",
       "2821         ▁note -2818\n",
       "3561            IT -3558\n",
       "1127         ▁comm -1124\n",
       "1232         ▁song -1229\n",
       "2189       ▁sequel -2186\n",
       "1788         ances -1785\n",
       "1192   ▁disappoint -1189\n",
       "2509            Ch -2506"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vocab 파일을 df로 저장\n",
    "vocab_list = pd.read_csv('imdb.vocab', sep='\\t', header=None, quoting=csv.QUOTE_NONE)\n",
    "vocab_list.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1c1c696e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7c4e45c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model 파일을 로드하여 인코딩 및 디코딩 작업 수행 가능\n",
    "sp = spm.SentencePieceProcessor()\n",
    "vocab_file = \"imdb.model\"\n",
    "sp.load(vocab_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27370df9",
   "metadata": {},
   "source": [
    "인코딩\n",
    "- encode_as_pieces : 문장을 입력하면 서브 워드 시퀀스로 변환\n",
    "- encode_as_ids : 문장을 입력하면 정수 시퀀스로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1271d195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I didn't at all think of it this way.\n",
      "['▁I', '▁didn', \"'\", 't', '▁at', '▁all', '▁think', '▁of', '▁it', '▁this', '▁way', '.']\n",
      "[41, 623, 4950, 4926, 138, 169, 378, 30, 58, 73, 413, 4945]\n",
      "\n",
      "I have waited a long time for someone to film\n",
      "['▁I', '▁have', '▁wa', 'ited', '▁a', '▁long', '▁time', '▁for', '▁someone', '▁to', '▁film']\n",
      "[41, 141, 1364, 1120, 4, 666, 285, 92, 1078, 33, 91]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lines = [\n",
    "  \"I didn't at all think of it this way.\",\n",
    "  \"I have waited a long time for someone to film\"\n",
    "]\n",
    "for line in lines:\n",
    "  print(line)\n",
    "  print(sp.encode_as_pieces(line))\n",
    "  print(sp.encode_as_ids(line))\n",
    "  print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abfb2caa",
   "metadata": {},
   "source": [
    "### 2. Word-piece\n",
    "- BertWordPieceTokenizer로 실습\n",
    "- 네이버 영화 리뷰 데이터 NSMC data 이용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "58f48be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import BertWordPieceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f2a4e6bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('ratings.txt', <http.client.HTTPMessage at 0x1f901253400>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NSMC data load\n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings.txt\", filename=\"ratings.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "54044e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# txt 파일을 df로 로드 후, 결측값 제거\n",
    "naver_df = pd.read_table('ratings.txt')\n",
    "naver_df = naver_df.dropna(how='any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "89b35903",
   "metadata": {},
   "outputs": [],
   "source": [
    "# word-piece의 입력으로 사용하기 위해 df를 txt로 저장\n",
    "with open('naver_review.txt', 'w', encoding='utf8') as f:\n",
    "    f.write('\\n'.join(naver_df['document']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b42a4f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BertWordPiece Tokenizer 설정\n",
    "word_tokenizer = BertWordPieceTokenizer(lowercase=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0fa3267f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NSMC 데이터 학습하여 단어 집합 얻기\n",
    "data_file = 'naver_review.txt'\n",
    "vocab_size = 30000\n",
    "limit_alphabet = 6000\n",
    "min_frequency = 5\n",
    "\n",
    "word_tokenizer.train(files=data_file,\n",
    "                vocab_size=vocab_size,\n",
    "                limit_alphabet=limit_alphabet,\n",
    "                min_frequency=min_frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bbb55730",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./vocab.txt']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vocab 저장\n",
    "word_tokenizer.save_model('./')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "48cfd6b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[PAD]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[UNK]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[CLS]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[SEP]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[MASK]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29995</th>\n",
       "      <td>말과</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29996</th>\n",
       "      <td>말들이</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29997</th>\n",
       "      <td>말라는</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29998</th>\n",
       "      <td>말밖에는</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29999</th>\n",
       "      <td>맘을</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30000 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0\n",
       "0       [PAD]\n",
       "1       [UNK]\n",
       "2       [CLS]\n",
       "3       [SEP]\n",
       "4      [MASK]\n",
       "...       ...\n",
       "29995      말과\n",
       "29996     말들이\n",
       "29997     말라는\n",
       "29998    말밖에는\n",
       "29999      맘을\n",
       "\n",
       "[30000 rows x 1 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vocab을 df로 로드\n",
    "### 앞서 vocab_size 30000으로 만들었기 때문에 vocab 안에 30000개 단어 존재\n",
    "df = pd.read_fwf('vocab.txt', header = None)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f6496a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "토큰화 결과 : ['아', '배고', '##픈', '##데', '짜장면', '##먹고', '##싶다']\n",
      "정수 인코딩 : [2111, 20631, 3372, 3327, 24681, 7873, 7378]\n",
      "디코딩 : 아 배고픈데 짜장면먹고싶다\n"
     ]
    }
   ],
   "source": [
    "# Word-piece 방식\n",
    "encoded = word_tokenizer.encode('아 배고픈데 짜장면먹고싶다')\n",
    "print('토큰화 결과 :',encoded.tokens)\n",
    "print('정수 인코딩 :',encoded.ids)\n",
    "print('디코딩 :',word_tokenizer.decode(encoded.ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "77e85341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "토큰화 결과 : ['아름다운', '대한민국', '금', '##수', '##강', '##산']\n",
      "정수 인코딩 : [6225, 7117, 720, 3257, 3727, 3788]\n",
      "디코딩 : 아름다운 대한민국 금수강산\n"
     ]
    }
   ],
   "source": [
    "# Word-piece 방식\n",
    "encoded2 = word_tokenizer.encode('아름다운 대한민국 금수강산')\n",
    "print('토큰화 결과 :',encoded2.tokens)\n",
    "print('정수 인코딩 :',encoded2.ids)\n",
    "print('디코딩 :',word_tokenizer.decode(encoded2.ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1481bb",
   "metadata": {},
   "source": [
    "### 3) NSMC로 sentence-piece 학습시키기 (비교 위해)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "62283dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence-piece로 각 단어에 고유한 정수 부여 (vocab 생성 과정)\n",
    "### vocab size 5000으로 지정\n",
    "spm.SentencePieceTrainer.Train('--input=naver_review.txt --model_prefix=rating --vocab_size=5000 --model_type=bpe --max_sentence_length=9999')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f6defb",
   "metadata": {},
   "source": [
    "#### -> vocab 생성이 완료되면, rating.model, rating.vocab 파일 2개가 생성된"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b9293ecb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>▁로</td>\n",
       "      <td>-288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3875</th>\n",
       "      <td>낫</td>\n",
       "      <td>-3872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1630</th>\n",
       "      <td>내가</td>\n",
       "      <td>-1627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3094</th>\n",
       "      <td>▁분들</td>\n",
       "      <td>-3091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1307</th>\n",
       "      <td>▁알게</td>\n",
       "      <td>-1304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3789</th>\n",
       "      <td>락</td>\n",
       "      <td>-3786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1573</th>\n",
       "      <td>▁봄</td>\n",
       "      <td>-1570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>▁8</td>\n",
       "      <td>-494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2292</th>\n",
       "      <td>or</td>\n",
       "      <td>-2289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326</th>\n",
       "      <td>ᄏᄏᄏ</td>\n",
       "      <td>-323</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        0     1\n",
       "291    ▁로  -288\n",
       "3875    낫 -3872\n",
       "1630   내가 -1627\n",
       "3094  ▁분들 -3091\n",
       "1307  ▁알게 -1304\n",
       "3789    락 -3786\n",
       "1573   ▁봄 -1570\n",
       "497    ▁8  -494\n",
       "2292   or -2289\n",
       "326   ᄏᄏᄏ  -323"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vocab 파일을 df로 저장\n",
    "vocab_list = pd.read_csv('rating.vocab', sep='\\t', header=None, quoting=csv.QUOTE_NONE)\n",
    "vocab_list.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "19c78165",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3d8e254f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model 파일을 로드하여 인코딩 및 디코딩 작업 수행 가능\n",
    "sp = spm.SentencePieceProcessor()\n",
    "vocab_file_NSMC = \"rating.model\"\n",
    "sp.load(vocab_file_NSMC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cc045e87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "아 배고픈데 짜장면먹고싶다\n",
      "['▁아', '▁배', '고', '픈', '데', '▁짜', '장면', '먹고', '싶다']\n",
      "[7, 81, 3280, 3726, 3316, 273, 557, 2607, 750]\n",
      "\n",
      "아름다운 대한민국 금수강산\n",
      "['▁아름다운', '▁대한민국', '▁금', '수', '강', '산']\n",
      "[694, 1862, 1712, 3335, 3529, 3645]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 인코딩 using sentence-piece 방식\n",
    "lines = [\n",
    "  \"아 배고픈데 짜장면먹고싶다\",\n",
    "  \"아름다운 대한민국 금수강산\"\n",
    "]\n",
    "for line in lines:\n",
    "  print(line)\n",
    "  print(sp.encode_as_pieces(line))\n",
    "  print(sp.encode_as_ids(line))\n",
    "  print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c585a31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b187ab42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31bc3d71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f979cf9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7a74ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec6ebd4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
